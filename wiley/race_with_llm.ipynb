{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahwiley-class/.pyenv/versions/3.10.1/envs/6.8200/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering CustomRace-v0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG, PPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "import numpy as np\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from openaikey import OPENAI_API_KEY\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "from custom_race import * \n",
    "import imageio\n",
    "from utils import record_videos, show_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32), {'speed': 10, 'crashed': False, 'action': array([0.49714988], dtype=float32), 'rewards': {'lane_centering_reward': 1.0, 'action_reward': 0.49714988, 'collision_reward': False, 'on_road_reward': True}})\n",
      "\n",
      "You are an assistant tasked with turning language subgoals into machine readable code. \n",
      "You are tasked with designing a reward function to incentivise a car to drive on a racetrack.\n",
      "You are given an action in the form [0.0, 0.0] where the first element is the steering angle and the second element is the throttle.\n",
      "You are also given a dictionary of:\n",
      "            \"lane_centering_reward\": 1/(1+self.config[\"lane_centering_cost\"]*lateral**2),\n",
      "            \"action_reward\": np.linalg.norm(action),\n",
      "            \"collision_reward\": self.vehicle.crashed,\n",
      "            \"on_road_reward\": self.vehicle.on_road,\n",
      "Please use all information to define a reward function that incentivizes the behaviors described in the following list.\n",
      "Please respond with only the code for the reward function which is defined by reward_func(self, action: np.ndarray, obs: {}) -> float:\n",
      "The reward function should address each point in the following list:\n",
      "To incentivize an agent to efficiently learn to drive and follow the race track environment shown in the image, while avoiding collisions and adhering to the lane, several key behaviors can be encouraged through the reward structures provided in the dictionary. Here are three specific behaviors to focus on:\n",
      "\n",
      "1. **Lane Centering Encouragement**:\n",
      "   - **Behavior**: The agent is incentivized to keep the car as centered as possible within the lane.\n",
      "   - **Implementation**: Utilize the \"lane_centering_reward\" which calculates a higher reward for maintaining the car closer to the center of the lane. The reward diminishes as the lateral distance from the lane center (indicated by variable `lateral`) increases. This reward structure naturally encourages the agent to minimize lateral deviation, promoting precise steering adjustments.\n",
      "\n",
      "2. **Smooth and Efficient Control Actions**:\n",
      "   - **Behavior**: Encourage the agent to perform smooth and minimal steering and throttle adjustments.\n",
      "   - **Implementation**: By employing the \"action_reward\", which is the norm of the action vector consisting of steering angle and throttle, the agent receives feedback encouraging minimal action magnitudes. This reward helps the agent learn to drive smoothly without making abrupt or large control inputs, which can be crucial for maintaining stability and efficiency, especially in curved sections of the track.\n",
      "\n",
      "3. **Collision Avoidance and Road Adherence**:\n",
      "   - **Behavior**: The agent must learn to avoid collisions and stay on the road.\n",
      "   - **Implementation**:\n",
      "     - **Collision Reward**: Using \"collision_reward\", penalize the agent heavily if it crashes (`self.vehicle.crashed`). This introduces a strong disincentive for hitting barriers or departing from the race track. \n",
      "     - **On-Road Reward**: Reward the agent for staying on the track through \"on_road_reward\". If the car remains on the road (`self.vehicle.on_road`), it continues to accrue points, effectively teaching the agent the importance of road adherence for continuous progress and success in the simulation.\n",
      "\n",
      "By combining these rewards, the learning environment can effectively guide the agent towards optimal behavior that includes staying centered in the lane, making judicious use of throttle and steering for efficient navigation, avoiding collisions, and consistently staying on the road. This strategic blend of incentivization aligns closely with the ultimate objective of following the track successfully while maintaining a smooth and safe driving style.\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "class YourRacecarSimulator:\n",
      "    def __init__(self, config):\n",
      "        self.config = config\n",
      "        self.vehicle = self.initialize_vehicle()\n",
      "\n",
      "    def initialize_vehicle(self):\n",
      "        \"\"\" A sample vehicle initialization for this example \"\"\"\n",
      "        vehicle = {\n",
      "            'crashed': False,\n",
      "            'on_road': True\n",
      "        }\n",
      "        return vehicle\n",
      "\n",
      "    def reward_func(self, action: np.ndarray, obs: {}) -> float:\n",
      "        # 1. Discourage straying too far from the lane center\n",
      "        lateral_deviation = obs.get('lateral_deviation', 0.0)  # Distance from lane center\n",
      "        lane_centering_cost = 1 / (1 + self.config[\"lane_centering_cost\"] * lateral_deviation ** 2)\n",
      "\n",
      "        # 2. Penalize high action values to prevent erratic behavior\n",
      "        action_magnitude = np.linalg.norm(action)\n",
      "\n",
      "        # 3. Strong penalty for collisions\n",
      "        collision_penalty = -500.0 if self.vehicle['crashed'] else 0.0\n",
      "\n",
      "        # 4. Reward for staying on the road\n",
      "        on_road_reward = 100.0 if self.vehicle['on_road'] else -100.0\n",
      "\n",
      "        # Combine all rewards to form final reward\n",
      "        total_reward = (lane_centering_cost \n",
      "                        - action_magnitude * self.config[\"action_cost\"]\n",
      "                        + collision_penalty \n",
      "                        + on_road_reward)\n",
      "        \n",
      "        return total_reward\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Create an OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "#getting render of the environment\n",
    "def compute_reward(self, achieved_goal: np.ndarray, desired_goal: np.ndarray, info: dict = None, p: float = 0.5) -> float:\n",
    "       return 0.0\n",
    "\n",
    "env = gym.make(\"CustomRace-v0\",  reward_func=compute_reward, render_mode=\"rgb_array\")\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "\n",
    "image_base64_str = env_render_to_base64(env)\n",
    "\n",
    "# PROMPTS FOR THE CLIENT\n",
    "env_description = \"\"\"\n",
    "Here is an image of a top-down view of a racecar on a race track environment. \n",
    "The yellow rectangle represents a car that can be controlled via throttle and steering angle.\n",
    "We would like the yellow car to follow the track and avoid collisions. \n",
    "We are given an action in the form [0.0, 0.0] where the first element is the steering angle and the second element is the throttle.\n",
    "We are also given a dictionary of:\n",
    "            \"lane_centering_reward\": 1/(1+self.config[\"lane_centering_cost\"]*lateral**2),\n",
    "            \"action_reward\": np.linalg.norm(action),\n",
    "            \"collision_reward\": self.vehicle.crashed,\n",
    "            \"on_road_reward\": self.vehicle.on_road,\n",
    "Please describe 3 behaviors in natural language that would incentivize an agent to efficiently learn drive in this environment desired.\n",
    "\"\"\"\n",
    "\n",
    "language_to_code_prompt = \"\"\"\n",
    "You are an assistant tasked with turning language subgoals into machine readable code. \n",
    "You are tasked with designing a reward function to incentivise a car to drive on a racetrack.\n",
    "You are given an action in the form [0.0, 0.0] where the first element is the steering angle and the second element is the throttle.\n",
    "You are also given a dictionary of:\n",
    "            \"lane_centering_reward\": 1/(1+self.config[\"lane_centering_cost\"]*lateral**2),\n",
    "            \"action_reward\": np.linalg.norm(action),\n",
    "            \"collision_reward\": self.vehicle.crashed,\n",
    "            \"on_road_reward\": self.vehicle.on_road,\n",
    "Please use all information to define a reward function that incentivizes the behaviors described in the following list.\n",
    "Please respond with only the code for the reward function which is defined by reward_func(self, action: np.ndarray, obs: {}) -> float:\n",
    "The reward function should address each point in the following list:\n",
    "\"\"\" \n",
    "# Our actions come in the form: [0.0, 0.0] where the first element is the steering angle and the second element is the throttle.\n",
    "natural_language = language_to_code_prompt + prompt_api_vision(client, env_description, image_base64_str)\n",
    "print(natural_language)\n",
    "code = prompt_api_no_vision(client, language_to_code_prompt)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speed': 10, 'crashed': False, 'action': array([-0.78005207], dtype=float32), 'rewards': {'lane_centering_reward': 1.0, 'action_reward': 0.78005207, 'collision_reward': False, 'on_road_reward': True}}\n",
      "Logging to wiley/log\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 522      |\n",
      "|    ep_rew_mean     | 14.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 94       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 767          |\n",
      "|    ep_rew_mean          | 14.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 92           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045223897 |\n",
      "|    clip_fraction        | 0.018        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.181       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.103        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 0.293        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+03     |\n",
      "|    ep_rew_mean          | 13.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032597347 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.534        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00563      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    std                  | 0.938        |\n",
      "|    value_loss           | 0.155        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.08e+03     |\n",
      "|    ep_rew_mean          | 16.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017483523 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.715        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0233       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0122      |\n",
      "|    std                  | 0.925        |\n",
      "|    value_loss           | 0.0609       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01e+03     |\n",
      "|    ep_rew_mean          | 24.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058753416 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.684        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.229        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0177      |\n",
      "|    std                  | 0.898        |\n",
      "|    value_loss           | 1.01         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 942          |\n",
      "|    ep_rew_mean          | 51.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047087697 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.403        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0121      |\n",
      "|    std                  | 0.881        |\n",
      "|    value_loss           | 1.66         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 982         |\n",
      "|    ep_rew_mean          | 63.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006206865 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.719       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.91        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    std                  | 0.872       |\n",
      "|    value_loss           | 9.17        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 798          |\n",
      "|    ep_rew_mean          | 64.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 89           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053712744 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.15         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    std                  | 0.864        |\n",
      "|    value_loss           | 6.59         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 828         |\n",
      "|    ep_rew_mean          | 95.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 206         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010140758 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.26        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    std                  | 0.859       |\n",
      "|    value_loss           | 12.7        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def reward_func(self, action: np.ndarray, obs: dict) -> float:\n",
    "        # 1. Discourage straying too far from the lane center\n",
    "        lateral_deviation = obs.get('lateral_deviation', 0.0)  # Distance from lane center\n",
    "        lane_centering_cost = 1 / (1 + self.config[\"lane_centering_cost\"] * lateral_deviation ** 2)\n",
    "\n",
    "        # 2. Penalize high action values to prevent erratic behavior\n",
    "        action_magnitude = np.linalg.norm(action)\n",
    "\n",
    "        # 3. Strong penalty for collisions\n",
    "        collision_penalty = -500.0 if self.vehicle['crashed'] else 0.0\n",
    "\n",
    "        # 4. Reward for staying on the road\n",
    "        on_road_reward = 100.0 if self.vehicle['on_road'] else -100.0\n",
    "\n",
    "        # Combine all rewards to form final reward\n",
    "        total_reward = (lane_centering_cost \n",
    "                        - action_magnitude * self.config[\"action_cost\"]\n",
    "                        + collision_penalty \n",
    "                        + on_road_reward)\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "mode = \"USE_PPO\"\n",
    "training_steps =100000\n",
    "\n",
    "env = gym.make(\"CustomRace-v0\",  reward_func=compute_reward, render_mode=\"rgb_array\")\n",
    "obs = env.reset()\n",
    "print(obs[1])\n",
    "\n",
    "\n",
    "# FOR DDPG\n",
    "if mode == \"USE_PPO\":\n",
    "    # set up logger\n",
    "    tmp_path = \"wiley/log\" # PATH FOR LOGGING\n",
    "    new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "    #set noise for ddpg\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "    # creating model and assign logger\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.set_logger(new_logger)\n",
    "    model.learn(total_timesteps=training_steps)\n",
    "    model.save(\"wiley/models/PPO_race_ai_reward\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6.8200",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
